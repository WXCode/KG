{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplyhired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)   \n",
    "df = pd.DataFrame()   \n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "for page in range(1,92): \n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_BA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,16): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_DS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,36): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_WA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1,57): \n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics+&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_MA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,48): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics+&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_RA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,11): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_FA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,66): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Philadelphia%2C+PA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Philadelphia'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Philadelphia_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=New+York%2C+NY&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'NewYork'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_NY_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,82): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,20): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,55): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,58): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics%E2%80%A9&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,19): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist&l=Los+Angeles%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'LosAngeles'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_LA_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,80): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,17): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,58): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,83): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,44): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist&l=Seattle%2C+WA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Seattle'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Seattle_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,18): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,86): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,55): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist&l=San+Francisco%2C+CA&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'SanFrancisco'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_SF_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Chicago&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,17): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,18): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,61): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=optiolns)\n",
    "\n",
    "\n",
    "for page in range(1,21): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist%E2%80%A9&l=Chicago%2C+IL&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Chicago'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Chicago_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/guorong/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,75): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,41): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,7): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,25): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,25): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,19): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,7): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist%E2%80%A9&l=Houston%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Houston'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Houston_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,92): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=business+analytics&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_BA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,67): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=operations+analyst&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_OA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,12): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=financial+analyst&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_FA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,59): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=risk+analytics&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,53): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=marketing+analytics&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,42): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=web+analytics&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_WA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "pd.set_option('max_colwidth',500)    # to remove column limit (Otherwise, we'll lose some info)\n",
    "df = pd.DataFrame()   # create a new data frame\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome('/Users/ravid/Downloads/chromedriver',options=options)\n",
    "\n",
    "\n",
    "for page in range(1,14): # page from 2 to 102\n",
    "    web = \"https://www.simplyhired.com/search?q=data+scientist%E2%80%A9&l=Dallas%2C+TX&pn=\"+str(page) \n",
    "    PAGE = requests.get(web)\n",
    "    target = BeautifulSoup(PAGE.content, 'html.parser')\n",
    "    results = target.find(id='job-list')\n",
    "    BA_jobs = results.find_all('h2', class_='jobposting-title')\n",
    "    for BA_job in BA_jobs:\n",
    "        job_link = BA_job.find('a')['href']\n",
    "        job_url = \"https://www.simplyhired.com\" + job_link\n",
    "        \n",
    "        driver.get(job_url)\n",
    "        time.sleep(0.5)\n",
    "        job_title_tag = driver.find_element_by_tag_name(\"h1\")\n",
    "        job_title = job_title_tag.text.strip() if job_title_tag else \"NONE\"\n",
    "        \n",
    "        \n",
    "        jd_tag = driver.find_element_by_xpath(\"//div[@class='viewjob-description ViewJob-description']\")\n",
    "        information_link = jd_tag.get_attribute('innerHTML')\n",
    "        target = BeautifulSoup(information_link, 'html.parser')\n",
    "        jd = target.text.strip()\n",
    "        \n",
    "        \n",
    "        df = df.append({'job_title': job_title,'job_information': jd,'job_location':'Dallas'}, ignore_index=True)\n",
    "\n",
    "job_data = df.to_csv(\"SimplyHired_Dallas_DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
